# 성림학사 세탁기 사용량 예측 AI

**1. 서론**

현재 성림학사 내 세탁 시설은 IoT 기능이 전무한 구형 장비로 작동되고 있다. 이로 인해 거주 학생들은 세탁기가 비어 있는지 확인하기 위하여 세탁실로 직접 가 확인해야 하는 불편함을 겪고 있다. 이를 해결하기 위하여 성림학사의 결제 정보 데이터를 시간대별 세탁기 사용량 데이터로 전처리하고, 이를 학습시켜 특정 시간대에 세탁기 사용량이 얼마일지 예측하는 AI를 만들고자 한다.

**2. 설계 과정**

최근 1달간 결제 정보 데이터를 성림학사의 결제금액 당 사용 시간에 따라 5분 간격 당 세탁기 사용량 데이터로 전처리하는 과정을 거쳤다.
또한 전처리한 데이터를 인공신경망에 집어넣어 학습시키는 과정을 반복하였다.

**3. 사용 언어 & 라이브러리**

Python, Tensorflow + Keras, Pandas, Numpy

**4. 상세 과정**

  ** 초기 데이터 형식
 
  <img width="566" height="519" alt="image" src="https://github.com/user-attachments/assets/68c77379-bdec-4aad-953d-92480f3d48b8" />

  1) 기본 설정

    계산 과정의 편의성을 위해 시간과 관련된 항목의 값들은 전부 'datetime' 객체로 변환

    초기 데이터 : df_before

    결제시간-종료시간 데이터 : df

    5분 간격 시간-세탁기 사용량 데이터(최종 데이터) : time

  2) 데이터 전처리

    (1) '매출일' 항목의 값들을 변환 (ex) 2025-04-02-00:00 -> 2025-04-02 00:00)

    (2) '결제시간' 항목을 기반으로 추가 결제금액('이용액' 항목의 값) 100원당 10분이 추가됨을 이용해 '종료시간' 항목, 항목 안 값들 생성. 생성한 값 통해 df 생성.

    (3) 5분 간격마다의 시간이 마지막 종료 시간보다 작거나 같을 때까지 5분 간격의 시간대 추가. (time의 첫번째 항목 채우는 과정)

    (4) 시간대가 특정 인덱스의 결제시간과 종료시간 사이에 있다면 그 인덱스에 포함되는 세탁기 사용량 1 증가. .(time의 두번째 항목 채우는 과정)

  3) AI 제작

    (1) 전처리한 데이터 불러오고 세탁기 수를 one-hot incoding 형태로 변환, 인공신경망은 datetime 객체를 인식하지 못하므로 시간, 분, 요일, 월 단위로 '결제시간' 항목의 data들을 쪼개고 각각의 값에 sin, cos함수를 넣은 값들을 열에 추가(따라서 4*2 = 8개의 열(feature) 추가).

    (2) 전처리 데이터를 X와 y로 변환(X : 입력 데이터, y : 정답 데이터)

    (3) Input layer : 8 node, hidden layer : 4 layer, 64 node, output layer : 14 node(세탁기 수가 14대)인 인공신경망을 학습.

  4) 사용 근거

    (1) Datetime 객체 값 변환할 때 sin, cos함수 사용한 이유
    
      : 날짜는 주기성을 갖고 있으며, 그냥 일, 월 등을 숫자로 변환해 집어넣으면 주기성은 무시한 채 잘못 해석할 위험(ex : 12월 다음은 1월이여서 가깝지만 단순히 숫자 자체로는 머므로 가까운 관계가 아니라 생각함)이 존재하므로 연속성과 주기성을 인공신경망에 강조하기 위하여 사용하였다.

    (2) 회귀 아닌 분류 사용한 이유

      : 세탁기 사용 수를 정확한 정수 단위로 예측해야 했기 때문이다.

    (3) Hidden layer 활성화함수로 LeakyReLU(alpha=0.01) 사용한 근거

      : sigmoid나 tanh를 activate function으로 사용하면 gradient vanishing의 위험이 있다. 또한 위험을 없애기 위해 ReLU를 사용하면 노드들이 죽는 문제가 말생하므로 입력값이 0 이하일 때 y=0이 아닌 y=ax(여기서 a는 매우 작은 값, 보통 0.01)을 사용함으로 노드가 죽는 문제를 예방한 LeakyReLU를 차용하였다.

    (4) 가중치 초기화 사용한 이유, He 초기화를 사용한 이유

      : 가중치 초기화로 가중치 대칭성 꺠기, 기울기 소실/폭주 방지, 분산 유지의 장점이 있기 때문이다. 활성화함수로 LeakyReLU를 사용했기 때문이다

**5. 출력 결과**

  <img width="635" height="444" alt="image" src="https://github.com/user-attachments/assets/83f97e13-db16-4291-b5f4-219ba0538836" />

  <img width="635" height="444" alt="image" src="https://github.com/user-attachments/assets/ff1688b0-a93d-4d9e-b242-e7464c27ce48" />

  <img width="590" height="460" alt="image" src="https://github.com/user-attachments/assets/80c64405-3a2a-42c0-8190-e34c1519caef" />

  <img width="590" height="460" alt="image" src="https://github.com/user-attachments/assets/09ec5d59-3642-47f9-ad50-8ca2d9c27556" />

**6. 한계점 및 개선방안**

  1) 제대로 학습되지 않았다.

    -> 데이터 자체에는 문제가 없었고, 단지 1)번에서도 말할 하이퍼파라미터 조정 문제 때문에 제대로 학습이 진행되지 않았던 것 같다.

  2) 충분한 학습으로 하이퍼파라미터를 제대로 설정하지 못했다.

    -> 시간을 들여 grid search와 같은 방식을 사용한다.

  3) 데이터의 표본이 최근 1달로 한정되어 일반성이 떨어진다.

    -> 사감 선생님에게 요청해 데이터를 일일히 사진으로 찍어가 직접 작업하는 방식 등등을 사용한다.

  4) Input data의 feature 수가 너무 적다.

    -> 날씨 정보 등을 data에 추가한다.

**7. 결론**

너무나도 좋은 data를 제공받아 활용할 수 있었지만 시간의 부족으로 완성도 있는 AI를 만들지 못했던 것 같아 아쉬웠다. 특히 가장 중요한 학습이 제대로 이루어지지 않는 부분을 제대로 개선하지 못한 것이 너무나도 아쉬웠다. 그렇지만 AI를 만들면서 고민하고 학습했던 과정 속에 얻어간 내용들이 많았던 것 같다.
